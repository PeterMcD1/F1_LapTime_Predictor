{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a19097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.parser import parse\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a806793",
   "metadata": {},
   "outputs": [],
   "source": [
    "###TODO: expand to full data, figure out how to visualize the difference in compounds or just train multiple neural networks on each compound, fix the size error idk why thats happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcacfe7-8fe5-44df-a6d5-cf26d849058d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b9face-adef-40de-8430-b3f147039a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"LapAndWeather/LapWeather_Australian Grand Prix.csv\",\"LapAndWeather/LapWeather_Austrian Grand Prix.csv\",\"LapAndWeather/LapWeather_Azerbaijan Grand Prix.csv\",\"LapAndWeather/LapWeather_Bahrain Grand Prix.csv\",\"LapAndWeather/LapWeather_Belgian Grand Prix.csv\",\"LapAndWeather/LapWeather_Brazilian Grand Prix.csv\",\"LapAndWeather/LapWeather_British Grand Prix.csv\",\"LapAndWeather/LapWeather_Canadian Grand Prix.csv\", \"LapAndWeather/LapWeather_Chinese Grand Prix.csv\",\"LapAndWeather/LapWeather_French Grand Prix.csv\",\"LapAndWeather/LapWeather_German Grand Prix.csv\",\"LapAndWeather/LapWeather_Hungarian Grand Prix.csv\",\"LapAndWeather/LapWeather_Italian Grand Prix.csv\",\"LapAndWeather/LapWeather_Japanese Grand Prix.csv\",\"LapAndWeather/LapWeather_Mexican Grand Prix.csv\",\"LapAndWeather/LapWeather_Monaco Grand Prix.csv\",\"LapAndWeather/LapWeather_Russian Grand Prix.csv\",\"LapAndWeather/LapWeather_Singapore Grand Prix.csv\",\"LapAndWeather/LapWeather_Spanish Grand Prix.csv\",\"LapAndWeather/LapWeather_United States Grand Prix.csv\"]\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in filenames ])\n",
    "# print(combined_csv)\n",
    "combined_csv.to_csv( \"combined_csv.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d63abe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17258\n",
      "13743\n",
      "13743\n",
      "13743\n",
      "13633\n",
      "13633\n",
      "13552\n",
      "13552\n",
      "13429\n",
      "13429\n",
      "13417\n",
      "13401\n",
      "13401\n",
      "13388\n",
      "13364\n",
      "13196\n",
      "13064\n",
      "13064\n",
      "13064\n",
      "13064\n",
      "torch.Size([13064, 1])\n",
      "[[ 2.   6.   4.  ... 38.2  3.8  1. ]\n",
      " [ 3.   6.   5.  ... 36.7  4.3  1. ]\n",
      " [ 4.   6.   6.  ... 36.8  2.9  1. ]\n",
      " ...\n",
      " [65.   6.  52.  ... 34.3  1.4 16. ]\n",
      " [66.   6.  53.  ... 34.1  1.5 16. ]\n",
      " [67.   6.  54.  ... 34.3  1.7 16. ]]\n",
      "[ 90.177  89.61   89.54  ... 100.552 100.83  103.413]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Temp/ipykernel_9624/3737578177.py:29: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  indexes = df.loc[df[\"Track\"]==i][((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)].index\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('combined_csv.csv')\n",
    "df = df[df[\"IsAccurate\"]==True]\n",
    "df = df[df[\"TrackStatus\"]==1]\n",
    "df = df.drop(['Time','DriverNumber','LapStartDate', 'WindDirection','Unnamed: 0', 'PitOutTime', 'PitInTime', 'Sector1Time','Sector2Time','Sector3Time','Sector1SessionTime','Sector2SessionTime','Sector3SessionTime','SpeedI1','SpeedI2','SpeedST','IsPersonalBest','FreshTyre','SpeedFL','LapStartTime','Driver','TrackStatus','IsAccurate'], axis=1)\n",
    "df = df.dropna()\n",
    "\n",
    "dfOutput = df['LapTime']\n",
    "train_targets = dfOutput.to_numpy()\n",
    "\n",
    "for i in range(len(train_targets)):\n",
    "    train_targets[i] = train_targets[i].replace('0 days ', '')\n",
    "# print(train_targets)\n",
    "actual_train_targets = []\n",
    "for time in train_targets:\n",
    "    td = parse(time) - parse('00:00:00')\n",
    "    seconds = td.total_seconds()\n",
    "    actual_train_targets.append(seconds)\n",
    "# print(actual_train_targets)\n",
    "dfLapTime = pd.DataFrame(actual_train_targets)\n",
    "\n",
    "df['LapTime'] = dfLapTime\n",
    "print(len(df))\n",
    "df = df.dropna()\n",
    "cols = ['LapTime']\n",
    "for i in range(1,20):\n",
    "    print(len(df))\n",
    "    Q1 = df.loc[df[\"Track\"]==i][cols].quantile(0.25)\n",
    "    Q3 = df.loc[df[\"Track\"]==i][cols].quantile(0.75)\n",
    "    IQR = Q3-Q1\n",
    "    indexes = df.loc[df[\"Track\"]==i][((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)].index\n",
    "    df.drop(indexes,inplace=True)\n",
    "    \n",
    "dfInput = df.drop(['LapTime'], axis=1)\n",
    "dfInput = dfInput.replace({'SUPERHARD':1, 'HARD':2, 'MEDIUM':3, 'SOFT':4,'SUPERSOFT': 5, 'ULTRASOFT': 6,'HYPERSOFT':7,\"INTERMEDIATE\":0,\"WET\":-1})\n",
    "dfInput = dfInput.replace({'Ferrari':1,'Mercedes':2,'Red Bull Racing':3, 'McLaren':4, 'Renault':5, 'Force India':6, 'Sauber':7, 'Williams':8, 'Toro Rosso':9, 'Haas F1 Team':10, 'Racing Point':6})\n",
    "train_inputs = dfInput.to_numpy()\n",
    "inputs_array = train_inputs.astype('float64')\n",
    "dfOutput = df['LapTime']\n",
    "\n",
    "targets_array = dfOutput.to_numpy()\n",
    "inputs = torch.Tensor(inputs_array)\n",
    "targets = torch.Tensor(targets_array)\n",
    "\n",
    "new_shape = (len(targets_array), 1)\n",
    "targets = targets.view(new_shape)\n",
    "print(targets.shape)\n",
    "\n",
    "print(inputs_array)\n",
    "print(targets_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03e8c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f1a0694",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_percent = 0.2\n",
    "num_rows = len(dfInput.index)\n",
    "val_size = int(num_rows * val_percent)\n",
    "train_size = num_rows - val_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da61690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle = True, num_workers = 0)\n",
    "val_loader = DataLoader(val_ds, batch_size, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fccaa518",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(inputs[0])\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a60896f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(12, 144),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(144,72),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(72,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.layers(xb)                       \n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        inputs, targets = batch \n",
    "        # Generate predictions\n",
    "        out = self(inputs)          \n",
    "        # Calcuate loss\n",
    "        mse_loss = nn.MSELoss()\n",
    "        # cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        # loss = cross_entropy_loss(out, targets)\n",
    "        loss = mse_loss(out, targets)  \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        inputs, targets = batch\n",
    "        # Generate predictions\n",
    "        out = self(inputs)\n",
    "        # Calculate loss\n",
    "        mse_loss = nn.MSELoss()\n",
    "        loss = mse_loss(out, targets)    \n",
    "        return {'val_loss': loss.detach()}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        return {'val_loss': epoch_loss.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result, num_epochs):\n",
    "        # Print result every 20th epoch\n",
    "        if (epoch+1) % 20 == 0 or epoch == num_epochs-1:\n",
    "            print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch+1, result['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c67ce36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc374939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c8906d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result, epochs)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8c986ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 10871.609375}\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(model, val_loader)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c4a3ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 196.7692\n",
      "Epoch [40], val_loss: 192.3611\n",
      "Epoch [60], val_loss: 191.1792\n",
      "Epoch [80], val_loss: 186.4618\n",
      "Epoch [100], val_loss: 186.6690\n",
      "Epoch [120], val_loss: 182.7290\n",
      "Epoch [140], val_loss: 180.8031\n",
      "Epoch [160], val_loss: 179.1771\n",
      "Epoch [180], val_loss: 177.7448\n",
      "Epoch [200], val_loss: 177.2711\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "lr = 1e-7\n",
    "history1 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e18488a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(input, target, model):\n",
    "    inputs = input.unsqueeze(0)\n",
    "    predictions = model(inputs)                \n",
    "    prediction = predictions[0].detach()\n",
    "    return target, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "603f4338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([71.5500]), tensor([86.0019]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, target = val_ds[12]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab4dcbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([89.8090]), tensor([84.0255]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, target = val_ds[50]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bed00bcf-8db3-471c-b4a6-116730336d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predict,actual):\n",
    "        predict = predict.detach().numpy()\n",
    "        actual = actual.numpy()\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for i in range(len(actual)):\n",
    "            if predict[i] >= actual[i]*.9 and predict[i] <= actual[i]*1.1:\n",
    "                count+=1\n",
    "        return count/len(predict)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0bd0a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "predictions = []\n",
    "for i in range(len(val_ds)):\n",
    "    input, target = val_ds[i]\n",
    "    target, prediction = predict_single(input, target, model)\n",
    "    # np.append(targets,target)\n",
    "    targets.append(target)\n",
    "    predictions.append(prediction)\n",
    "    # np.append(predictions,prediction)\n",
    "\n",
    "# print(len(predictions))\n",
    "# print(targets[0])\n",
    "accuracy = []\n",
    "for j in range(len(predictions)-1):\n",
    "    accuracy.append(get_accuracy(predictions[j],targets[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bc3ce23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.496937212863706\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(accuracy)/len(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbb1ef5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# ax.scatter(range(len(predictions)),targets)\n",
    "# ax.scatter(range(len(predictions)), predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9964388e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9f1f59101e07bffb7c2ecfaca1a3c7ffe3cd326ee75e914ab1b038684b38c5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
