{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a19097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.parser import parse\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a806793",
   "metadata": {},
   "outputs": [],
   "source": [
    "###TODO: expand to full data, figure out how to visualize the difference in compounds or just train multiple neural networks on each compound, fix the size error idk why thats happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcacfe7-8fe5-44df-a6d5-cf26d849058d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b9face-adef-40de-8430-b3f147039a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"LapAndWeather/LapWeather_Australian Grand Prix.csv\",\"LapAndWeather/LapWeather_Austrian Grand Prix.csv\",\"LapAndWeather/LapWeather_Azerbaijan Grand Prix.csv\",\"LapAndWeather/LapWeather_Bahrain Grand Prix.csv\",\"LapAndWeather/LapWeather_Belgian Grand Prix.csv\",\"LapAndWeather/LapWeather_Brazilian Grand Prix.csv\",\"LapAndWeather/LapWeather_British Grand Prix.csv\",\"LapAndWeather/LapWeather_Canadian Grand Prix.csv\", \"LapAndWeather/LapWeather_Chinese Grand Prix.csv\",\"LapAndWeather/LapWeather_French Grand Prix.csv\",\"LapAndWeather/LapWeather_German Grand Prix.csv\",\"LapAndWeather/LapWeather_Hungarian Grand Prix.csv\",\"LapAndWeather/LapWeather_Italian Grand Prix.csv\",\"LapAndWeather/LapWeather_Japanese Grand Prix.csv\",\"LapAndWeather/LapWeather_Mexican Grand Prix.csv\",\"LapAndWeather/LapWeather_Monaco Grand Prix.csv\",\"LapAndWeather/LapWeather_Russian Grand Prix.csv\",\"LapAndWeather/LapWeather_Singapore Grand Prix.csv\",\"LapAndWeather/LapWeather_Spanish Grand Prix.csv\",\"LapAndWeather/LapWeather_United States Grand Prix.csv\"]\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in filenames ])\n",
    "# print(combined_csv)\n",
    "combined_csv.to_csv( \"combined_csv.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d63abe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13743\n",
      "13743\n",
      "13743\n",
      "13633\n",
      "13633\n",
      "13552\n",
      "13552\n",
      "13429\n",
      "13429\n",
      "13417\n",
      "13401\n",
      "13401\n",
      "13388\n",
      "13364\n",
      "13196\n",
      "13064\n",
      "13064\n",
      "13064\n",
      "13064\n",
      "       LapTime  LapNumber   Compound  TyreLife        Team  AirTemp  Humidity  \\\n",
      "1       90.177          2  ULTRASOFT       4.0     Ferrari     24.2      36.3   \n",
      "2       89.610          3  ULTRASOFT       5.0     Ferrari     23.9      36.5   \n",
      "3       89.540          4  ULTRASOFT       6.0     Ferrari     23.9      36.3   \n",
      "4       89.481          5  ULTRASOFT       7.0     Ferrari     23.5      36.3   \n",
      "7       88.405          8  ULTRASOFT      10.0     Ferrari     23.8      35.6   \n",
      "...        ...        ...        ...       ...         ...      ...       ...   \n",
      "17253  100.578         63  ULTRASOFT      50.0  Toro Rosso     26.2      57.1   \n",
      "17254  100.566         64  ULTRASOFT      51.0  Toro Rosso     26.2      57.0   \n",
      "17255  100.552         65  ULTRASOFT      52.0  Toro Rosso     26.2      57.0   \n",
      "17256  100.830         66  ULTRASOFT      53.0  Toro Rosso     26.1      57.1   \n",
      "17257  103.413         67  ULTRASOFT      54.0  Toro Rosso     25.9      58.1   \n",
      "\n",
      "       Pressure  Rainfall  TrackTemp  WindSpeed  Track  \n",
      "1         996.9     False       38.2        3.8      1  \n",
      "2         997.1     False       36.7        4.3      1  \n",
      "3         997.1     False       36.8        2.9      1  \n",
      "4         997.2     False       36.4        2.5      1  \n",
      "7         997.0     False       37.5        2.9      1  \n",
      "...         ...       ...        ...        ...    ...  \n",
      "17253    1015.3     False       34.3        1.2     16  \n",
      "17254    1015.3     False       34.3        1.4     16  \n",
      "17255    1015.3     False       34.3        1.4     16  \n",
      "17256    1015.3     False       34.1        1.5     16  \n",
      "17257    1015.2     False       34.3        1.7     16  \n",
      "\n",
      "[13064 rows x 12 columns]\n",
      "torch.Size([13064, 1])\n",
      "[[ 2.   6.   4.  ... 38.2  3.8  1. ]\n",
      " [ 3.   6.   5.  ... 36.7  4.3  1. ]\n",
      " [ 4.   6.   6.  ... 36.8  2.9  1. ]\n",
      " ...\n",
      " [65.   6.  52.  ... 34.3  1.4 16. ]\n",
      " [66.   6.  53.  ... 34.1  1.5 16. ]\n",
      " [67.   6.  54.  ... 34.3  1.7 16. ]]\n",
      "[ 90.177  89.61   89.54  ... 100.552 100.83  103.413]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Temp/ipykernel_15480/1073215378.py:29: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  indexes = df.loc[df[\"Track\"]==i][((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)].index\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('combined_csv.csv')\n",
    "df = df[df[\"IsAccurate\"]==True]\n",
    "df = df[df[\"TrackStatus\"]==1]\n",
    "df = df.drop(['Time','DriverNumber','LapStartDate','Stint', 'WindDirection','Unnamed: 0', 'PitOutTime', 'PitInTime', 'Sector1Time','Sector2Time','Sector3Time','Sector1SessionTime','Sector2SessionTime','Sector3SessionTime','SpeedI1','SpeedI2','SpeedST','IsPersonalBest','FreshTyre','SpeedFL','LapStartTime','Driver','TrackStatus','IsAccurate'], axis=1)\n",
    "df = df.dropna()\n",
    "\n",
    "dfOutput = df['LapTime']\n",
    "train_targets = dfOutput.to_numpy()\n",
    "\n",
    "for i in range(len(train_targets)):\n",
    "    train_targets[i] = train_targets[i].replace('0 days ', '')\n",
    "# print(train_targets)\n",
    "actual_train_targets = []\n",
    "for time in train_targets:\n",
    "    td = parse(time) - parse('00:00:00')\n",
    "    seconds = td.total_seconds()\n",
    "    actual_train_targets.append(seconds)\n",
    "# print(actual_train_targets)\n",
    "dfLapTime = pd.DataFrame(actual_train_targets)\n",
    "\n",
    "df['LapTime'] = dfLapTime\n",
    "df = df.dropna()\n",
    "cols = ['LapTime']\n",
    "for i in range(1,20):\n",
    "    print(len(df))\n",
    "    Q1 = df.loc[df[\"Track\"]==i][cols].quantile(0.25)\n",
    "    Q3 = df.loc[df[\"Track\"]==i][cols].quantile(0.75)\n",
    "    IQR = Q3-Q1\n",
    "    indexes = df.loc[df[\"Track\"]==i][((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)].index\n",
    "    df.drop(indexes,inplace=True)\n",
    "    \n",
    "dfInput = df.drop(['LapTime'], axis=1)\n",
    "dfInput = dfInput.replace({'SUPERHARD':1, 'HARD':2, 'MEDIUM':3, 'SOFT':4,'SUPERSOFT': 5, 'ULTRASOFT': 6,'HYPERSOFT':7,\"INTERMEDIATE\":0,\"WET\":-1})\n",
    "dfInput = dfInput.replace({'Ferrari':1,'Mercedes':2,'Red Bull Racing':3, 'McLaren':4, 'Renault':5, 'Force India':6, 'Sauber':7, 'Williams':8, 'Toro Rosso':9, 'Haas F1 Team':10, 'Racing Point':6})\n",
    "train_inputs = dfInput.to_numpy()\n",
    "inputs_array = train_inputs.astype('float64')\n",
    "dfOutput = df['LapTime']\n",
    "\n",
    "print(df)\n",
    "targets_array = dfOutput.to_numpy()\n",
    "inputs = torch.Tensor(inputs_array)\n",
    "targets = torch.Tensor(targets_array)\n",
    "\n",
    "new_shape = (len(targets_array), 1)\n",
    "targets = targets.view(new_shape)\n",
    "print(targets.shape)\n",
    "\n",
    "print(inputs_array)\n",
    "print(targets_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03e8c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f1a0694",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_percent = .7\n",
    "num_rows = len(dfInput.index)\n",
    "val_size = int(num_rows * val_percent)\n",
    "train_size = num_rows - val_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da61690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle = True, num_workers = 0)\n",
    "val_loader = DataLoader(val_ds, batch_size, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fccaa518",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(inputs[0])\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a60896f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(11, 144),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(144,72),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(72,36),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(36,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.layers(xb)                       \n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        inputs, targets = batch \n",
    "        # Generate predictions\n",
    "        out = self(inputs)          \n",
    "        # Calcuate loss\n",
    "        mse_loss = nn.MSELoss()\n",
    "        # cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        # loss = cross_entropy_loss(out, targets)\n",
    "        loss = mse_loss(out, targets)  \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        inputs, targets = batch\n",
    "        # Generate predictions\n",
    "        out = self(inputs)\n",
    "        # Calculate loss\n",
    "        mse_loss = nn.MSELoss()\n",
    "        loss = mse_loss(out, targets)    \n",
    "        return {'val_loss': loss.detach()}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        return {'val_loss': epoch_loss.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result, num_epochs):\n",
    "        # Print result every 20th epoch\n",
    "        if (epoch+1) % 20 == 0 or epoch == num_epochs-1:\n",
    "            print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch+1, result['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c67ce36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc374939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c8906d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result, epochs)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8c986ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 10971.2626953125}\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(model, val_loader)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c4a3ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 196.5171\n",
      "Epoch [40], val_loss: 194.8229\n",
      "Epoch [60], val_loss: 190.0933\n",
      "Epoch [80], val_loss: 187.6328\n",
      "Epoch [100], val_loss: 190.7870\n",
      "Epoch [120], val_loss: 193.4475\n",
      "Epoch [140], val_loss: 185.0018\n",
      "Epoch [160], val_loss: 181.4832\n",
      "Epoch [180], val_loss: 181.7081\n",
      "Epoch [200], val_loss: 179.8624\n",
      "Epoch [220], val_loss: 179.3873\n",
      "Epoch [240], val_loss: 177.4143\n",
      "Epoch [260], val_loss: 176.4210\n",
      "Epoch [280], val_loss: 187.0483\n",
      "Epoch [300], val_loss: 185.0712\n",
      "Epoch [320], val_loss: 176.3417\n",
      "Epoch [340], val_loss: 171.6704\n",
      "Epoch [360], val_loss: 172.5883\n",
      "Epoch [380], val_loss: 176.0434\n",
      "Epoch [400], val_loss: 167.9630\n",
      "Epoch [420], val_loss: 167.0015\n",
      "Epoch [440], val_loss: 165.5471\n",
      "Epoch [460], val_loss: 164.8899\n",
      "Epoch [480], val_loss: 176.0278\n",
      "Epoch [500], val_loss: 164.6415\n",
      "Epoch [520], val_loss: 168.1738\n",
      "Epoch [540], val_loss: 160.7907\n",
      "Epoch [560], val_loss: 162.9585\n",
      "Epoch [580], val_loss: 157.2756\n",
      "Epoch [600], val_loss: 158.8082\n",
      "Epoch [620], val_loss: 156.7922\n",
      "Epoch [640], val_loss: 156.9885\n",
      "Epoch [660], val_loss: 156.9286\n",
      "Epoch [680], val_loss: 170.0897\n",
      "Epoch [700], val_loss: 152.4462\n",
      "Epoch [720], val_loss: 156.7594\n",
      "Epoch [740], val_loss: 150.0991\n",
      "Epoch [760], val_loss: 153.2494\n",
      "Epoch [780], val_loss: 172.9543\n",
      "Epoch [800], val_loss: 180.4909\n",
      "Epoch [820], val_loss: 146.5015\n",
      "Epoch [840], val_loss: 145.5132\n",
      "Epoch [860], val_loss: 155.1209\n",
      "Epoch [880], val_loss: 145.7048\n",
      "Epoch [900], val_loss: 144.7354\n",
      "Epoch [920], val_loss: 141.8026\n",
      "Epoch [940], val_loss: 145.7826\n",
      "Epoch [960], val_loss: 142.3165\n",
      "Epoch [980], val_loss: 136.9709\n",
      "Epoch [1000], val_loss: 133.2380\n",
      "Epoch [1020], val_loss: 132.0787\n",
      "Epoch [1040], val_loss: 131.2549\n",
      "Epoch [1060], val_loss: 134.9685\n",
      "Epoch [1080], val_loss: 126.6844\n",
      "Epoch [1100], val_loss: 123.9826\n",
      "Epoch [1120], val_loss: 117.9065\n",
      "Epoch [1140], val_loss: 123.1064\n",
      "Epoch [1160], val_loss: 121.9372\n",
      "Epoch [1180], val_loss: 152.5472\n",
      "Epoch [1200], val_loss: 121.3079\n",
      "Epoch [1220], val_loss: 107.2812\n",
      "Epoch [1240], val_loss: 106.0257\n",
      "Epoch [1260], val_loss: 116.8592\n",
      "Epoch [1280], val_loss: 101.9067\n",
      "Epoch [1300], val_loss: 100.7926\n",
      "Epoch [1320], val_loss: 100.5992\n",
      "Epoch [1340], val_loss: 99.6473\n",
      "Epoch [1360], val_loss: 99.3456\n",
      "Epoch [1380], val_loss: 101.8958\n",
      "Epoch [1400], val_loss: 97.2321\n",
      "Epoch [1420], val_loss: 102.9116\n",
      "Epoch [1440], val_loss: 96.8440\n",
      "Epoch [1460], val_loss: 94.9411\n",
      "Epoch [1480], val_loss: 99.5313\n",
      "Epoch [1500], val_loss: 114.0891\n",
      "Epoch [1520], val_loss: 117.1011\n",
      "Epoch [1540], val_loss: 96.2009\n",
      "Epoch [1560], val_loss: 93.5405\n",
      "Epoch [1580], val_loss: 115.6746\n",
      "Epoch [1600], val_loss: 97.2646\n",
      "Epoch [1620], val_loss: 93.0232\n",
      "Epoch [1640], val_loss: 92.3769\n",
      "Epoch [1660], val_loss: 131.1899\n",
      "Epoch [1680], val_loss: 93.8531\n",
      "Epoch [1700], val_loss: 92.9725\n",
      "Epoch [1720], val_loss: 94.0375\n",
      "Epoch [1740], val_loss: 92.1392\n",
      "Epoch [1760], val_loss: 91.3069\n",
      "Epoch [1780], val_loss: 91.9582\n",
      "Epoch [1800], val_loss: 90.6777\n",
      "Epoch [1820], val_loss: 90.1814\n",
      "Epoch [1840], val_loss: 93.9788\n",
      "Epoch [1860], val_loss: 91.9597\n",
      "Epoch [1880], val_loss: 93.0950\n",
      "Epoch [1900], val_loss: 146.6987\n",
      "Epoch [1920], val_loss: 90.1200\n",
      "Epoch [1940], val_loss: 102.8690\n",
      "Epoch [1960], val_loss: 104.3009\n",
      "Epoch [1980], val_loss: 112.1358\n",
      "Epoch [2000], val_loss: 92.4244\n",
      "Epoch [2020], val_loss: 89.1086\n",
      "Epoch [2040], val_loss: 104.2046\n",
      "Epoch [2060], val_loss: 90.5008\n",
      "Epoch [2080], val_loss: 92.0148\n",
      "Epoch [2100], val_loss: 90.9249\n",
      "Epoch [2120], val_loss: 105.3443\n",
      "Epoch [2140], val_loss: 89.7546\n",
      "Epoch [2160], val_loss: 92.9234\n",
      "Epoch [2180], val_loss: 91.8728\n",
      "Epoch [2200], val_loss: 103.6957\n",
      "Epoch [2220], val_loss: 87.7631\n",
      "Epoch [2240], val_loss: 91.1834\n",
      "Epoch [2260], val_loss: 88.2861\n",
      "Epoch [2280], val_loss: 87.6088\n",
      "Epoch [2300], val_loss: 91.4448\n",
      "Epoch [2320], val_loss: 87.5518\n",
      "Epoch [2340], val_loss: 90.7362\n",
      "Epoch [2360], val_loss: 87.4582\n",
      "Epoch [2380], val_loss: 87.2135\n",
      "Epoch [2400], val_loss: 91.3553\n",
      "Epoch [2420], val_loss: 88.2440\n",
      "Epoch [2440], val_loss: 88.4778\n",
      "Epoch [2460], val_loss: 86.9357\n",
      "Epoch [2480], val_loss: 89.8714\n",
      "Epoch [2500], val_loss: 86.2956\n",
      "Epoch [2520], val_loss: 87.2324\n",
      "Epoch [2540], val_loss: 90.7993\n",
      "Epoch [2560], val_loss: 96.7355\n",
      "Epoch [2580], val_loss: 87.6037\n",
      "Epoch [2600], val_loss: 85.1319\n",
      "Epoch [2620], val_loss: 85.9760\n",
      "Epoch [2640], val_loss: 99.2850\n",
      "Epoch [2660], val_loss: 86.1680\n",
      "Epoch [2680], val_loss: 85.8301\n",
      "Epoch [2700], val_loss: 92.0497\n",
      "Epoch [2720], val_loss: 85.8270\n",
      "Epoch [2740], val_loss: 86.9914\n",
      "Epoch [2760], val_loss: 86.4720\n",
      "Epoch [2780], val_loss: 85.1437\n",
      "Epoch [2800], val_loss: 89.3116\n",
      "Epoch [2820], val_loss: 85.6432\n",
      "Epoch [2840], val_loss: 84.0765\n",
      "Epoch [2860], val_loss: 85.0304\n",
      "Epoch [2880], val_loss: 87.0672\n",
      "Epoch [2900], val_loss: 84.2816\n",
      "Epoch [2920], val_loss: 84.1211\n",
      "Epoch [2940], val_loss: 85.7443\n",
      "Epoch [2960], val_loss: 83.6802\n",
      "Epoch [2980], val_loss: 86.7343\n",
      "Epoch [3000], val_loss: 84.2440\n",
      "Epoch [3020], val_loss: 82.2741\n",
      "Epoch [3040], val_loss: 82.6885\n",
      "Epoch [3060], val_loss: 83.5189\n",
      "Epoch [3080], val_loss: 82.4514\n",
      "Epoch [3100], val_loss: 85.6812\n",
      "Epoch [3120], val_loss: 84.0893\n",
      "Epoch [3140], val_loss: 82.4060\n",
      "Epoch [3160], val_loss: 83.2204\n",
      "Epoch [3180], val_loss: 89.3296\n",
      "Epoch [3200], val_loss: 89.2772\n",
      "Epoch [3220], val_loss: 83.9092\n",
      "Epoch [3240], val_loss: 83.3114\n",
      "Epoch [3260], val_loss: 82.8470\n",
      "Epoch [3280], val_loss: 83.5884\n",
      "Epoch [3300], val_loss: 81.0583\n",
      "Epoch [3320], val_loss: 81.0535\n",
      "Epoch [3340], val_loss: 80.2778\n",
      "Epoch [3360], val_loss: 81.8396\n",
      "Epoch [3380], val_loss: 81.0330\n",
      "Epoch [3400], val_loss: 82.9881\n",
      "Epoch [3420], val_loss: 83.5676\n",
      "Epoch [3440], val_loss: 79.7904\n",
      "Epoch [3460], val_loss: 82.9815\n",
      "Epoch [3480], val_loss: 79.0646\n",
      "Epoch [3500], val_loss: 78.7856\n",
      "Epoch [3520], val_loss: 80.3789\n",
      "Epoch [3540], val_loss: 78.0519\n",
      "Epoch [3560], val_loss: 79.4442\n",
      "Epoch [3580], val_loss: 78.9385\n",
      "Epoch [3600], val_loss: 78.8315\n",
      "Epoch [3620], val_loss: 79.2713\n",
      "Epoch [3640], val_loss: 79.2482\n",
      "Epoch [3660], val_loss: 78.9427\n",
      "Epoch [3680], val_loss: 80.7186\n",
      "Epoch [3700], val_loss: 77.6360\n",
      "Epoch [3720], val_loss: 78.2867\n",
      "Epoch [3740], val_loss: 77.0591\n",
      "Epoch [3760], val_loss: 76.8630\n",
      "Epoch [3780], val_loss: 76.9031\n",
      "Epoch [3800], val_loss: 75.6145\n",
      "Epoch [3820], val_loss: 77.6547\n",
      "Epoch [3840], val_loss: 75.6607\n",
      "Epoch [3860], val_loss: 77.8810\n",
      "Epoch [3880], val_loss: 75.1550\n",
      "Epoch [3900], val_loss: 75.7227\n",
      "Epoch [3920], val_loss: 75.3253\n",
      "Epoch [3940], val_loss: 76.6028\n",
      "Epoch [3960], val_loss: 76.9640\n",
      "Epoch [3980], val_loss: 74.9469\n",
      "Epoch [4000], val_loss: 74.7806\n",
      "Epoch [4020], val_loss: 75.9989\n",
      "Epoch [4040], val_loss: 78.0321\n",
      "Epoch [4060], val_loss: 74.2135\n",
      "Epoch [4080], val_loss: 87.6086\n",
      "Epoch [4100], val_loss: 80.9821\n",
      "Epoch [4120], val_loss: 76.9307\n",
      "Epoch [4140], val_loss: 85.6721\n",
      "Epoch [4160], val_loss: 76.5126\n",
      "Epoch [4180], val_loss: 77.7328\n",
      "Epoch [4200], val_loss: 74.2660\n",
      "Epoch [4220], val_loss: 81.1622\n",
      "Epoch [4240], val_loss: 74.5314\n",
      "Epoch [4260], val_loss: 76.7409\n",
      "Epoch [4280], val_loss: 72.6671\n",
      "Epoch [4300], val_loss: 73.6784\n",
      "Epoch [4320], val_loss: 73.7945\n",
      "Epoch [4340], val_loss: 72.9674\n",
      "Epoch [4360], val_loss: 78.7921\n",
      "Epoch [4380], val_loss: 75.2585\n",
      "Epoch [4400], val_loss: 74.1226\n",
      "Epoch [4420], val_loss: 75.2661\n",
      "Epoch [4440], val_loss: 74.3192\n",
      "Epoch [4460], val_loss: 72.9760\n",
      "Epoch [4480], val_loss: 72.8427\n",
      "Epoch [4500], val_loss: 73.9269\n",
      "Epoch [4520], val_loss: 74.0464\n",
      "Epoch [4540], val_loss: 72.5529\n",
      "Epoch [4560], val_loss: 74.2992\n",
      "Epoch [4580], val_loss: 72.1622\n",
      "Epoch [4600], val_loss: 75.5472\n",
      "Epoch [4620], val_loss: 80.2124\n",
      "Epoch [4640], val_loss: 76.7294\n",
      "Epoch [4660], val_loss: 78.2066\n",
      "Epoch [4680], val_loss: 72.1002\n",
      "Epoch [4700], val_loss: 72.4449\n",
      "Epoch [4720], val_loss: 71.3001\n",
      "Epoch [4740], val_loss: 71.3763\n",
      "Epoch [4760], val_loss: 74.3824\n",
      "Epoch [4780], val_loss: 70.8683\n",
      "Epoch [4800], val_loss: 73.1537\n",
      "Epoch [4820], val_loss: 74.1200\n",
      "Epoch [4840], val_loss: 70.7146\n",
      "Epoch [4860], val_loss: 72.9471\n",
      "Epoch [4880], val_loss: 75.0307\n",
      "Epoch [4900], val_loss: 72.7894\n",
      "Epoch [4920], val_loss: 70.1273\n",
      "Epoch [4940], val_loss: 70.7357\n",
      "Epoch [4960], val_loss: 70.0933\n",
      "Epoch [4980], val_loss: 74.3769\n",
      "Epoch [5000], val_loss: 71.0499\n",
      "Epoch [5020], val_loss: 69.6677\n",
      "Epoch [5040], val_loss: 74.4834\n",
      "Epoch [5060], val_loss: 71.0415\n",
      "Epoch [5080], val_loss: 68.6793\n",
      "Epoch [5100], val_loss: 79.7700\n",
      "Epoch [5120], val_loss: 74.2308\n",
      "Epoch [5140], val_loss: 71.7753\n",
      "Epoch [5160], val_loss: 72.0056\n",
      "Epoch [5180], val_loss: 71.2248\n",
      "Epoch [5200], val_loss: 71.0424\n",
      "Epoch [5220], val_loss: 74.6305\n",
      "Epoch [5240], val_loss: 69.3773\n",
      "Epoch [5260], val_loss: 70.7279\n",
      "Epoch [5280], val_loss: 69.4433\n",
      "Epoch [5300], val_loss: 72.0342\n",
      "Epoch [5320], val_loss: 75.3478\n",
      "Epoch [5340], val_loss: 69.8433\n",
      "Epoch [5360], val_loss: 69.0991\n",
      "Epoch [5380], val_loss: 69.0258\n",
      "Epoch [5400], val_loss: 72.2350\n",
      "Epoch [5420], val_loss: 73.7871\n",
      "Epoch [5440], val_loss: 71.2922\n",
      "Epoch [5460], val_loss: 71.5383\n",
      "Epoch [5480], val_loss: 68.3533\n",
      "Epoch [5500], val_loss: 68.0381\n",
      "Epoch [5520], val_loss: 76.3197\n",
      "Epoch [5540], val_loss: 69.4126\n",
      "Epoch [5560], val_loss: 75.5928\n",
      "Epoch [5580], val_loss: 67.7953\n",
      "Epoch [5600], val_loss: 69.3379\n",
      "Epoch [5620], val_loss: 69.2492\n",
      "Epoch [5640], val_loss: 73.6559\n",
      "Epoch [5660], val_loss: 69.2277\n",
      "Epoch [5680], val_loss: 67.8710\n",
      "Epoch [5700], val_loss: 66.7039\n",
      "Epoch [5720], val_loss: 69.3206\n",
      "Epoch [5740], val_loss: 66.8050\n",
      "Epoch [5760], val_loss: 66.6297\n",
      "Epoch [5780], val_loss: 71.2025\n",
      "Epoch [5800], val_loss: 71.6097\n",
      "Epoch [5820], val_loss: 68.7633\n",
      "Epoch [5840], val_loss: 67.4571\n",
      "Epoch [5860], val_loss: 71.6949\n",
      "Epoch [5880], val_loss: 67.1416\n",
      "Epoch [5900], val_loss: 68.9628\n",
      "Epoch [5920], val_loss: 75.7180\n",
      "Epoch [5940], val_loss: 69.3555\n",
      "Epoch [5960], val_loss: 67.1268\n",
      "Epoch [5980], val_loss: 66.6009\n",
      "Epoch [6000], val_loss: 68.9096\n",
      "Epoch [6020], val_loss: 71.9770\n",
      "Epoch [6040], val_loss: 71.5015\n",
      "Epoch [6060], val_loss: 70.7059\n",
      "Epoch [6080], val_loss: 65.7713\n",
      "Epoch [6100], val_loss: 67.4763\n",
      "Epoch [6120], val_loss: 65.3297\n",
      "Epoch [6140], val_loss: 72.8397\n",
      "Epoch [6160], val_loss: 65.0293\n",
      "Epoch [6180], val_loss: 65.5618\n",
      "Epoch [6200], val_loss: 71.9809\n",
      "Epoch [6220], val_loss: 69.7601\n",
      "Epoch [6240], val_loss: 65.0171\n",
      "Epoch [6260], val_loss: 74.7168\n",
      "Epoch [6280], val_loss: 65.8073\n",
      "Epoch [6300], val_loss: 67.2036\n",
      "Epoch [6320], val_loss: 66.9322\n",
      "Epoch [6340], val_loss: 66.5298\n",
      "Epoch [6360], val_loss: 66.0948\n",
      "Epoch [6380], val_loss: 67.9258\n",
      "Epoch [6400], val_loss: 69.3084\n",
      "Epoch [6420], val_loss: 65.4339\n",
      "Epoch [6440], val_loss: 68.4065\n",
      "Epoch [6460], val_loss: 80.3448\n",
      "Epoch [6480], val_loss: 64.4878\n",
      "Epoch [6500], val_loss: 67.1460\n",
      "Epoch [6520], val_loss: 64.6289\n",
      "Epoch [6540], val_loss: 73.5149\n",
      "Epoch [6560], val_loss: 67.1637\n",
      "Epoch [6580], val_loss: 65.5558\n",
      "Epoch [6600], val_loss: 63.8607\n",
      "Epoch [6620], val_loss: 66.8985\n",
      "Epoch [6640], val_loss: 65.8826\n",
      "Epoch [6660], val_loss: 66.7517\n",
      "Epoch [6680], val_loss: 67.0459\n",
      "Epoch [6700], val_loss: 63.3989\n",
      "Epoch [6720], val_loss: 64.0222\n",
      "Epoch [6740], val_loss: 64.8482\n",
      "Epoch [6760], val_loss: 69.5956\n",
      "Epoch [6780], val_loss: 69.7291\n",
      "Epoch [6800], val_loss: 64.3726\n",
      "Epoch [6820], val_loss: 63.0011\n",
      "Epoch [6840], val_loss: 64.0136\n",
      "Epoch [6860], val_loss: 67.7837\n",
      "Epoch [6880], val_loss: 64.1719\n",
      "Epoch [6900], val_loss: 66.4834\n",
      "Epoch [6920], val_loss: 64.6054\n",
      "Epoch [6940], val_loss: 64.3648\n",
      "Epoch [6960], val_loss: 64.4718\n",
      "Epoch [6980], val_loss: 63.6090\n",
      "Epoch [7000], val_loss: 63.8986\n"
     ]
    }
   ],
   "source": [
    "epochs = 7000\n",
    "lr = 1e-6\n",
    "history1 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e18488a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(input, target, model):\n",
    "    inputs = input.unsqueeze(0)\n",
    "    predictions = model(inputs)                \n",
    "    prediction = predictions[0].detach()\n",
    "    return target, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "603f4338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([97.1670]), tensor([92.0836]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, target = val_ds[12]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab4dcbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([79.8790]), tensor([78.9445]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, target = val_ds[50]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bed00bcf-8db3-471c-b4a6-116730336d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predict,actual):\n",
    "        predict = predict.detach().numpy()\n",
    "        actual = actual.numpy()\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for i in range(len(actual)):\n",
    "            if predict[i] >= actual[i]*.9 and predict[i] <= actual[i]*1.1:\n",
    "                count+=1\n",
    "        return count/len(predict)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0bd0a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "predictions = []\n",
    "for i in range(len(val_ds)):\n",
    "    input, target = val_ds[i]\n",
    "    target, prediction = predict_single(input, target, model)\n",
    "    # np.append(targets,target)\n",
    "    targets.append(target)\n",
    "    predictions.append(prediction)\n",
    "    # np.append(predictions,prediction)\n",
    "\n",
    "# print(len(predictions))\n",
    "# print(targets[0])\n",
    "accuracy = []\n",
    "for j in range(len(predictions)-1):\n",
    "    accuracy.append(get_accuracy(predictions[j],targets[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bc3ce23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within 10% of actual lap time:\n",
      "79.36125998031281\n"
     ]
    }
   ],
   "source": [
    "print(\"Within 10% of actual lap time:\")\n",
    "print(np.sum(accuracy)/len(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80e70bde-7e01-42a5-bba1-8fae24608b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"LapAndWeather2019/LapWeather_Australian Grand Prix.csv\",\"LapAndWeather/LapWeather_Austrian Grand Prix.csv\",\"LapAndWeather/LapWeather_Azerbaijan Grand Prix.csv\",\"LapAndWeather/LapWeather_Bahrain Grand Prix.csv\",\"LapAndWeather/LapWeather_Belgian Grand Prix.csv\",\"LapAndWeather/LapWeather_Brazilian Grand Prix.csv\",\"LapAndWeather/LapWeather_British Grand Prix.csv\",\"LapAndWeather/LapWeather_Canadian Grand Prix.csv\", \"LapAndWeather/LapWeather_Chinese Grand Prix.csv\",\"LapAndWeather/LapWeather_French Grand Prix.csv\",\"LapAndWeather/LapWeather_German Grand Prix.csv\",\"LapAndWeather/LapWeather_Hungarian Grand Prix.csv\",\"LapAndWeather/LapWeather_Italian Grand Prix.csv\",\"LapAndWeather/LapWeather_Japanese Grand Prix.csv\",\"LapAndWeather/LapWeather_Mexican Grand Prix.csv\",\"LapAndWeather/LapWeather_Monaco Grand Prix.csv\",\"LapAndWeather/LapWeather_Russian Grand Prix.csv\",\"LapAndWeather/LapWeather_Singapore Grand Prix.csv\",\"LapAndWeather/LapWeather_Spanish Grand Prix.csv\",\"LapAndWeather/LapWeather_United States Grand Prix.csv\"]\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in filenames ])\n",
    "# print(combined_csv)\n",
    "combined_csv.to_csv( \"combined_csv_2019.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9964388e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14174\n",
      "14090\n",
      "14090\n",
      "14031\n",
      "14031\n",
      "14031\n",
      "14031\n",
      "14031\n",
      "13917\n",
      "13904\n",
      "13895\n",
      "13895\n",
      "13731\n",
      "13714\n",
      "13714\n",
      "13704\n",
      "13704\n",
      "13704\n",
      "13704\n",
      "       LapTime  LapNumber   Compound  TyreLife      Team  AirTemp  Humidity  \\\n",
      "1       89.246          2       SOFT       5.0  Mercedes     23.6      69.7   \n",
      "2       88.641          3       SOFT       6.0  Mercedes     23.5      70.5   \n",
      "3       88.759          4       SOFT       7.0  Mercedes     23.6      70.3   \n",
      "4       89.034          5       SOFT       8.0  Mercedes     23.7      69.1   \n",
      "5       88.783          6       SOFT       9.0  Mercedes     23.6      69.4   \n",
      "...        ...        ...        ...       ...       ...      ...       ...   \n",
      "17527  100.578          8  ULTRASOFT      10.0   Ferrari     25.7      54.2   \n",
      "17528  100.566          9  ULTRASOFT      11.0   Ferrari     25.6      53.9   \n",
      "17529  100.552         10  ULTRASOFT      12.0   Ferrari     25.7      53.6   \n",
      "17530  100.830         11  ULTRASOFT      13.0   Ferrari     25.9      55.2   \n",
      "17531  103.413         12  ULTRASOFT      14.0   Ferrari     26.0      57.0   \n",
      "\n",
      "       Pressure  Rainfall  TrackTemp  WindSpeed  Track  \n",
      "1        1015.5     False       43.5        1.2      1  \n",
      "2        1015.4     False       43.4        1.3      1  \n",
      "3        1015.4     False       43.4        0.8      1  \n",
      "4        1015.4     False       43.0        1.2      1  \n",
      "5        1015.3     False       43.0        1.6      1  \n",
      "...         ...       ...        ...        ...    ...  \n",
      "17527    1016.5     False       30.3        1.5     17  \n",
      "17528    1016.5     False       30.7        2.0     17  \n",
      "17529    1016.5     False       31.1        1.4     17  \n",
      "17530    1016.5     False       31.5        0.7     17  \n",
      "17531    1016.5     False       31.3        1.6     17  \n",
      "\n",
      "[13704 rows x 12 columns]\n",
      "torch.Size([13704, 1])\n",
      "[[ 2.   4.   5.  ... 43.5  1.2  1. ]\n",
      " [ 3.   4.   6.  ... 43.4  1.3  1. ]\n",
      " [ 4.   4.   7.  ... 43.4  0.8  1. ]\n",
      " ...\n",
      " [10.   6.  12.  ... 31.1  1.4 17. ]\n",
      " [11.   6.  13.  ... 31.5  0.7 17. ]\n",
      " [12.   6.  14.  ... 31.3  1.6 17. ]]\n",
      "[ 89.246  88.641  88.759 ... 100.552 100.83  103.413]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Temp/ipykernel_15480/3436083740.py:29: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  indexes = df.loc[df[\"Track\"]==i][((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)].index\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('combined_csv_2019.csv')\n",
    "df = df[df[\"IsAccurate\"]==True]\n",
    "df = df[df[\"TrackStatus\"]==1]\n",
    "df = df.drop(['Time','DriverNumber','LapStartDate','Stint', 'WindDirection','Unnamed: 0', 'PitOutTime', 'PitInTime', 'Sector1Time','Sector2Time','Sector3Time','Sector1SessionTime','Sector2SessionTime','Sector3SessionTime','SpeedI1','SpeedI2','SpeedST','IsPersonalBest','FreshTyre','SpeedFL','LapStartTime','Driver','TrackStatus','IsAccurate'], axis=1)\n",
    "df = df.dropna()\n",
    "\n",
    "dfOutput = df['LapTime']\n",
    "train_targets = dfOutput.to_numpy()\n",
    "\n",
    "for i in range(len(train_targets)):\n",
    "    train_targets[i] = train_targets[i].replace('0 days ', '')\n",
    "# print(train_targets)\n",
    "actual_train_targets = []\n",
    "for time in train_targets:\n",
    "    td = parse(time) - parse('00:00:00')\n",
    "    seconds = td.total_seconds()\n",
    "    actual_train_targets.append(seconds)\n",
    "# print(actual_train_targets)\n",
    "dfLapTime = pd.DataFrame(actual_train_targets)\n",
    "\n",
    "df['LapTime'] = dfLapTime\n",
    "df = df.dropna()\n",
    "cols = ['LapTime']\n",
    "for i in range(1,20):\n",
    "    print(len(df))\n",
    "    Q1 = df.loc[df[\"Track\"]==i][cols].quantile(0.25)\n",
    "    Q3 = df.loc[df[\"Track\"]==i][cols].quantile(0.75)\n",
    "    IQR = Q3-Q1\n",
    "    indexes = df.loc[df[\"Track\"]==i][((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)].index\n",
    "    df.drop(indexes,inplace=True)\n",
    "    \n",
    "dfInput = df.drop(['LapTime'], axis=1)\n",
    "dfInput = dfInput.replace({'SUPERHARD':1, 'HARD':2, 'MEDIUM':3, 'SOFT':4,'SUPERSOFT': 5, 'ULTRASOFT': 6,'HYPERSOFT':7,\"INTERMEDIATE\":0,\"WET\":-1})\n",
    "dfInput = dfInput.replace({'Ferrari':1,'Mercedes':2,'Red Bull Racing':3, 'McLaren':4, 'Renault':5, 'Force India':6, 'Alfa Romeo Racing':7, 'Sauber':7, 'Williams':8, 'Toro Rosso':9, 'Haas F1 Team':10, 'Racing Point':6})\n",
    "train_inputs = dfInput.to_numpy()\n",
    "inputs_array = train_inputs.astype('float64')\n",
    "dfOutput = df['LapTime']\n",
    "\n",
    "print(df)\n",
    "targets_array = dfOutput.to_numpy()\n",
    "inputs = torch.Tensor(inputs_array)\n",
    "targets = torch.Tensor(targets_array)\n",
    "\n",
    "new_shape = (len(targets_array), 1)\n",
    "targets = targets.view(new_shape)\n",
    "print(targets.shape)\n",
    "\n",
    "print(inputs_array)\n",
    "print(targets_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc4e5801-e311-47e0-8963-10c80bee804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 63.89863204956055}\n"
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(inputs, targets)\n",
    "val_percent = 1\n",
    "num_rows = len(dfInput.index)\n",
    "val_size = int(num_rows * val_percent)\n",
    "train_size = num_rows - val_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "result = evaluate(model, val_loader)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e679d78f-7a47-4f6a-b07c-610684942475",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "predictions = []\n",
    "for i in range(len(val_ds)):\n",
    "    input, target = val_ds[i]\n",
    "    target, prediction = predict_single(input, target, model)\n",
    "    # np.append(targets,target)\n",
    "    targets.append(target)\n",
    "    predictions.append(prediction)\n",
    "    # np.append(predictions,prediction)\n",
    "\n",
    "# print(len(predictions))\n",
    "# print(targets[0])\n",
    "accuracy = []\n",
    "for j in range(len(predictions)-1):\n",
    "    accuracy.append(get_accuracy(predictions[j],targets[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3b721ae-65c5-4825-88b2-f0bda9d055ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within 10% of actual lap time:\n",
      "69.43734948551412\n"
     ]
    }
   ],
   "source": [
    "print(\"Within 10% of actual lap time:\")\n",
    "print(np.sum(accuracy)/len(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9f1f59101e07bffb7c2ecfaca1a3c7ffe3cd326ee75e914ab1b038684b38c5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
