{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a19097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.parser import parse\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a806793",
   "metadata": {},
   "outputs": [],
   "source": [
    "###TODO: expand to full data, figure out how to visualize the difference in compounds or just train multiple neural networks on each compound, fix the size error idk why thats happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcacfe7-8fe5-44df-a6d5-cf26d849058d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b9face-adef-40de-8430-b3f147039a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"LapAndWeather/LapWeather_Australian Grand Prix.csv\",\"LapAndWeather/LapWeather_Austrian Grand Prix.csv\",\"LapAndWeather/LapWeather_Azerbaijan Grand Prix.csv\",\"LapAndWeather/LapWeather_Bahrain Grand Prix.csv\",\"LapAndWeather/LapWeather_Belgian Grand Prix.csv\",\"LapAndWeather/LapWeather_Brazilian Grand Prix.csv\",\"LapAndWeather/LapWeather_British Grand Prix.csv\",\"LapAndWeather/LapWeather_Canadian Grand Prix.csv\", \"LapAndWeather/LapWeather_Chinese Grand Prix.csv\",\"LapAndWeather/LapWeather_French Grand Prix.csv\",\"LapAndWeather/LapWeather_German Grand Prix.csv\",\"LapAndWeather/LapWeather_Hungarian Grand Prix.csv\",\"LapAndWeather/LapWeather_Italian Grand Prix.csv\",\"LapAndWeather/LapWeather_Japanese Grand Prix.csv\",\"LapAndWeather/LapWeather_Mexican Grand Prix.csv\",\"LapAndWeather/LapWeather_Monaco Grand Prix.csv\",\"LapAndWeather/LapWeather_Russian Grand Prix.csv\",\"LapAndWeather/LapWeather_Singapore Grand Prix.csv\",\"LapAndWeather/LapWeather_Spanish Grand Prix.csv\",\"LapAndWeather/LapWeather_United States Grand Prix.csv\"]\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in filenames ])\n",
    "# print(combined_csv)\n",
    "combined_csv.to_csv( \"combined_csv.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d63abe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13743\n",
      "13743\n",
      "13743\n",
      "13633\n",
      "13633\n",
      "13552\n",
      "13552\n",
      "13429\n",
      "13429\n",
      "13417\n",
      "13401\n",
      "13401\n",
      "13388\n",
      "13364\n",
      "13196\n",
      "13064\n",
      "13064\n",
      "13064\n",
      "13064\n",
      "       LapTime  LapNumber   Compound  TyreLife        Team  AirTemp  Humidity  \\\n",
      "1       90.177          2  ULTRASOFT       4.0     Ferrari     24.2      36.3   \n",
      "2       89.610          3  ULTRASOFT       5.0     Ferrari     23.9      36.5   \n",
      "3       89.540          4  ULTRASOFT       6.0     Ferrari     23.9      36.3   \n",
      "4       89.481          5  ULTRASOFT       7.0     Ferrari     23.5      36.3   \n",
      "7       88.405          8  ULTRASOFT      10.0     Ferrari     23.8      35.6   \n",
      "...        ...        ...        ...       ...         ...      ...       ...   \n",
      "17253  100.578         63  ULTRASOFT      50.0  Toro Rosso     26.2      57.1   \n",
      "17254  100.566         64  ULTRASOFT      51.0  Toro Rosso     26.2      57.0   \n",
      "17255  100.552         65  ULTRASOFT      52.0  Toro Rosso     26.2      57.0   \n",
      "17256  100.830         66  ULTRASOFT      53.0  Toro Rosso     26.1      57.1   \n",
      "17257  103.413         67  ULTRASOFT      54.0  Toro Rosso     25.9      58.1   \n",
      "\n",
      "       Pressure  Rainfall  TrackTemp  WindSpeed  Track  \n",
      "1         996.9     False       38.2        3.8      1  \n",
      "2         997.1     False       36.7        4.3      1  \n",
      "3         997.1     False       36.8        2.9      1  \n",
      "4         997.2     False       36.4        2.5      1  \n",
      "7         997.0     False       37.5        2.9      1  \n",
      "...         ...       ...        ...        ...    ...  \n",
      "17253    1015.3     False       34.3        1.2     16  \n",
      "17254    1015.3     False       34.3        1.4     16  \n",
      "17255    1015.3     False       34.3        1.4     16  \n",
      "17256    1015.3     False       34.1        1.5     16  \n",
      "17257    1015.2     False       34.3        1.7     16  \n",
      "\n",
      "[13064 rows x 12 columns]\n",
      "torch.Size([13064, 1])\n",
      "[[ 2.   6.   4.  ... 38.2  3.8  1. ]\n",
      " [ 3.   6.   5.  ... 36.7  4.3  1. ]\n",
      " [ 4.   6.   6.  ... 36.8  2.9  1. ]\n",
      " ...\n",
      " [65.   6.  52.  ... 34.3  1.4 16. ]\n",
      " [66.   6.  53.  ... 34.1  1.5 16. ]\n",
      " [67.   6.  54.  ... 34.3  1.7 16. ]]\n",
      "[ 90.177  89.61   89.54  ... 100.552 100.83  103.413]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Temp/ipykernel_12136/1073215378.py:29: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  indexes = df.loc[df[\"Track\"]==i][((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)].index\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('combined_csv.csv')\n",
    "df = df[df[\"IsAccurate\"]==True]\n",
    "df = df[df[\"TrackStatus\"]==1]\n",
    "df = df.drop(['Time','DriverNumber','LapStartDate','Stint', 'WindDirection','Unnamed: 0', 'PitOutTime', 'PitInTime', 'Sector1Time','Sector2Time','Sector3Time','Sector1SessionTime','Sector2SessionTime','Sector3SessionTime','SpeedI1','SpeedI2','SpeedST','IsPersonalBest','FreshTyre','SpeedFL','LapStartTime','Driver','TrackStatus','IsAccurate'], axis=1)\n",
    "df = df.dropna()\n",
    "\n",
    "dfOutput = df['LapTime']\n",
    "train_targets = dfOutput.to_numpy()\n",
    "\n",
    "for i in range(len(train_targets)):\n",
    "    train_targets[i] = train_targets[i].replace('0 days ', '')\n",
    "# print(train_targets)\n",
    "actual_train_targets = []\n",
    "for time in train_targets:\n",
    "    td = parse(time) - parse('00:00:00')\n",
    "    seconds = td.total_seconds()\n",
    "    actual_train_targets.append(seconds)\n",
    "# print(actual_train_targets)\n",
    "dfLapTime = pd.DataFrame(actual_train_targets)\n",
    "\n",
    "df['LapTime'] = dfLapTime\n",
    "df = df.dropna()\n",
    "cols = ['LapTime']\n",
    "for i in range(1,20):\n",
    "    print(len(df))\n",
    "    Q1 = df.loc[df[\"Track\"]==i][cols].quantile(0.25)\n",
    "    Q3 = df.loc[df[\"Track\"]==i][cols].quantile(0.75)\n",
    "    IQR = Q3-Q1\n",
    "    indexes = df.loc[df[\"Track\"]==i][((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)].index\n",
    "    df.drop(indexes,inplace=True)\n",
    "    \n",
    "dfInput = df.drop(['LapTime'], axis=1)\n",
    "dfInput = dfInput.replace({'SUPERHARD':1, 'HARD':2, 'MEDIUM':3, 'SOFT':4,'SUPERSOFT': 5, 'ULTRASOFT': 6,'HYPERSOFT':7,\"INTERMEDIATE\":0,\"WET\":-1})\n",
    "dfInput = dfInput.replace({'Ferrari':1,'Mercedes':2,'Red Bull Racing':3, 'McLaren':4, 'Renault':5, 'Force India':6, 'Sauber':7, 'Williams':8, 'Toro Rosso':9, 'Haas F1 Team':10, 'Racing Point':6})\n",
    "train_inputs = dfInput.to_numpy()\n",
    "inputs_array = train_inputs.astype('float64')\n",
    "dfOutput = df['LapTime']\n",
    "\n",
    "print(df)\n",
    "targets_array = dfOutput.to_numpy()\n",
    "inputs = torch.Tensor(inputs_array)\n",
    "targets = torch.Tensor(targets_array)\n",
    "\n",
    "new_shape = (len(targets_array), 1)\n",
    "targets = targets.view(new_shape)\n",
    "print(targets.shape)\n",
    "\n",
    "print(inputs_array)\n",
    "print(targets_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03e8c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f1a0694",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_percent = 0.2\n",
    "num_rows = len(dfInput.index)\n",
    "val_size = int(num_rows * val_percent)\n",
    "train_size = num_rows - val_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da61690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle = True, num_workers = 0)\n",
    "val_loader = DataLoader(val_ds, batch_size, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fccaa518",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(inputs[0])\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a60896f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(11, 144),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(144,72),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(72,36),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(36,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.layers(xb)                       \n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        inputs, targets = batch \n",
    "        # Generate predictions\n",
    "        out = self(inputs)          \n",
    "        # Calcuate loss\n",
    "        mse_loss = nn.MSELoss()\n",
    "        # cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        # loss = cross_entropy_loss(out, targets)\n",
    "        loss = mse_loss(out, targets)  \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        inputs, targets = batch\n",
    "        # Generate predictions\n",
    "        out = self(inputs)\n",
    "        # Calculate loss\n",
    "        mse_loss = nn.MSELoss()\n",
    "        loss = mse_loss(out, targets)    \n",
    "        return {'val_loss': loss.detach()}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        return {'val_loss': epoch_loss.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result, num_epochs):\n",
    "        # Print result every 20th epoch\n",
    "        if (epoch+1) % 20 == 0 or epoch == num_epochs-1:\n",
    "            print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch+1, result['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c67ce36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc374939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c8906d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result, epochs)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8c986ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 9022.494140625}\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(model, val_loader)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c4a3ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 181.3377\n",
      "Epoch [40], val_loss: 176.2751\n",
      "Epoch [60], val_loss: 186.3605\n",
      "Epoch [80], val_loss: 195.5441\n",
      "Epoch [100], val_loss: 165.4651\n",
      "Epoch [120], val_loss: 160.7414\n",
      "Epoch [140], val_loss: 156.4713\n",
      "Epoch [160], val_loss: 174.8457\n",
      "Epoch [180], val_loss: 145.8118\n",
      "Epoch [200], val_loss: 143.7206\n",
      "Epoch [220], val_loss: 131.6835\n",
      "Epoch [240], val_loss: 120.0602\n",
      "Epoch [260], val_loss: 151.5976\n",
      "Epoch [280], val_loss: 127.2915\n",
      "Epoch [300], val_loss: 135.6454\n",
      "Epoch [320], val_loss: 112.7774\n",
      "Epoch [340], val_loss: 103.0408\n",
      "Epoch [360], val_loss: 103.8406\n",
      "Epoch [380], val_loss: 106.0595\n",
      "Epoch [400], val_loss: 98.3570\n",
      "Epoch [420], val_loss: 103.2561\n",
      "Epoch [440], val_loss: 100.0615\n",
      "Epoch [460], val_loss: 97.1386\n",
      "Epoch [480], val_loss: 97.5711\n",
      "Epoch [500], val_loss: 100.1755\n",
      "Epoch [520], val_loss: 96.2916\n",
      "Epoch [540], val_loss: 97.7658\n",
      "Epoch [560], val_loss: 95.8342\n",
      "Epoch [580], val_loss: 93.5943\n",
      "Epoch [600], val_loss: 101.8846\n",
      "Epoch [620], val_loss: 100.6936\n",
      "Epoch [640], val_loss: 94.3522\n",
      "Epoch [660], val_loss: 93.0968\n",
      "Epoch [680], val_loss: 106.5391\n",
      "Epoch [700], val_loss: 93.8700\n",
      "Epoch [720], val_loss: 88.9143\n",
      "Epoch [740], val_loss: 90.8298\n",
      "Epoch [760], val_loss: 89.9578\n",
      "Epoch [780], val_loss: 86.9339\n",
      "Epoch [800], val_loss: 87.9436\n",
      "Epoch [820], val_loss: 87.6026\n",
      "Epoch [840], val_loss: 87.7587\n",
      "Epoch [860], val_loss: 88.9745\n",
      "Epoch [880], val_loss: 82.3588\n",
      "Epoch [900], val_loss: 82.0750\n",
      "Epoch [920], val_loss: 82.4369\n",
      "Epoch [940], val_loss: 82.2758\n",
      "Epoch [960], val_loss: 79.9841\n",
      "Epoch [980], val_loss: 77.5072\n",
      "Epoch [1000], val_loss: 85.6300\n",
      "Epoch [1020], val_loss: 81.2600\n",
      "Epoch [1040], val_loss: 80.5204\n",
      "Epoch [1060], val_loss: 76.3498\n",
      "Epoch [1080], val_loss: 82.0176\n",
      "Epoch [1100], val_loss: 73.9799\n",
      "Epoch [1120], val_loss: 74.6984\n",
      "Epoch [1140], val_loss: 77.3680\n",
      "Epoch [1160], val_loss: 76.2121\n",
      "Epoch [1180], val_loss: 82.9914\n",
      "Epoch [1200], val_loss: 72.8420\n",
      "Epoch [1220], val_loss: 90.6101\n",
      "Epoch [1240], val_loss: 79.8957\n",
      "Epoch [1260], val_loss: 77.3660\n",
      "Epoch [1280], val_loss: 89.5639\n",
      "Epoch [1300], val_loss: 72.4820\n",
      "Epoch [1320], val_loss: 81.8399\n",
      "Epoch [1340], val_loss: 79.6318\n",
      "Epoch [1360], val_loss: 80.6207\n",
      "Epoch [1380], val_loss: 84.0422\n",
      "Epoch [1400], val_loss: 83.2942\n",
      "Epoch [1420], val_loss: 73.0141\n",
      "Epoch [1440], val_loss: 75.4194\n",
      "Epoch [1460], val_loss: 92.2768\n",
      "Epoch [1480], val_loss: 81.8982\n",
      "Epoch [1500], val_loss: 75.4427\n"
     ]
    }
   ],
   "source": [
    "epochs = 1500\n",
    "lr = 1e-6\n",
    "history1 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e18488a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(input, target, model):\n",
    "    inputs = input.unsqueeze(0)\n",
    "    predictions = model(inputs)                \n",
    "    prediction = predictions[0].detach()\n",
    "    return target, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "603f4338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([81.7880]), tensor([80.7008]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, target = val_ds[12]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab4dcbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([82.1950]), tensor([101.2538]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, target = val_ds[50]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bed00bcf-8db3-471c-b4a6-116730336d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predict,actual):\n",
    "        predict = predict.detach().numpy()\n",
    "        actual = actual.numpy()\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for i in range(len(actual)):\n",
    "            if predict[i] >= actual[i]*.9 and predict[i] <= actual[i]*1.1:\n",
    "                count+=1\n",
    "        return count/len(predict)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0bd0a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "predictions = []\n",
    "for i in range(len(val_ds)):\n",
    "    input, target = val_ds[i]\n",
    "    target, prediction = predict_single(input, target, model)\n",
    "    # np.append(targets,target)\n",
    "    targets.append(target)\n",
    "    predictions.append(prediction)\n",
    "    # np.append(predictions,prediction)\n",
    "\n",
    "# print(len(predictions))\n",
    "# print(targets[0])\n",
    "accuracy = []\n",
    "for j in range(len(predictions)-1):\n",
    "    accuracy.append(get_accuracy(predictions[j],targets[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bc3ce23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within 10% of actual lap time:\n",
      "74.87552661815397\n"
     ]
    }
   ],
   "source": [
    "print(\"Within 10% of actual lap time:\")\n",
    "print(np.sum(accuracy)/len(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbb1ef5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# ax.scatter(range(len(predictions)),targets)\n",
    "# ax.scatter(range(len(predictions)), predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9964388e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9f1f59101e07bffb7c2ecfaca1a3c7ffe3cd326ee75e914ab1b038684b38c5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
