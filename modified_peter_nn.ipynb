{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a19097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.parser import parse\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a806793",
   "metadata": {},
   "outputs": [],
   "source": [
    "###TODO: expand to full data, figure out how to visualize the difference in compounds or just train multiple neural networks on each compound, fix the size error idk why thats happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcacfe7-8fe5-44df-a6d5-cf26d849058d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b9face-adef-40de-8430-b3f147039a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"LapWeather_Australian Grand Prix.csv\",\"LapWeather_Austrian Grand Prix.csv\",\"LapWeather_Azerbaijan Grand Prix.csv\",\"LapWeather_Bahrain Grand Prix.csv\",\"LapWeather_Belgian Grand Prix.csv\",\"LapWeather_Brazilian Grand Prix.csv\",\"LapWeather_British Grand Prix.csv\",\"LapWeather_Canadian Grand Prix.csv\", \"LapWeather_Chinese Grand Prix.csv\",\"LapWeather_French Grand Prix.csv\",\"LapWeather_German Grand Prix.csv\",\"LapWeather_Hungarian Grand Prix.csv\",\"LapWeather_Italian Grand Prix.csv\",\"LapWeather_Japanese Grand Prix.csv\",\"LapWeather_Mexican Grand Prix.csv\",\"LapWeather_Monaco Grand Prix.csv\",\"LapWeather_Russian Grand Prix.csv\",\"LapWeather_Singapore Grand Prix.csv\",\"LapWeather_Spanish Grand Prix.csv\",\"LapWeather_United States Grand Prix.csv\"]\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in filenames ])\n",
    "combined_csv.to_csv( \"combined_csv.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d63abe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.    6.    4.  ...  38.2 296.    3.8]\n",
      " [  3.    6.    5.  ...  36.7 289.    4.3]\n",
      " [  4.    6.    6.  ...  36.8 255.    2.9]\n",
      " ...\n",
      " [  6.    5.    9.  ...  30.3  87.    3. ]\n",
      " [  7.    5.   10.  ...  30.5  89.    2.7]\n",
      " [  8.    5.   11.  ...  30.3  99.    2.2]]\n",
      "[ 90.177  89.61   89.54  ... 100.552 100.83  103.413]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('combined_csv.csv')\n",
    "df = df.drop(['Time','DriverNumber','LapStartDate', 'Unnamed: 0', 'PitOutTime', 'PitInTime', 'Sector1Time','Sector2Time','Sector3Time','Sector1SessionTime','Sector2SessionTime','Sector3SessionTime','SpeedI1','SpeedI2','SpeedST','IsPersonalBest','FreshTyre','SpeedFL','LapStartTime','Team','Driver','TrackStatus','IsAccurate'], axis=1)\n",
    "df = df.dropna()\n",
    "\n",
    "dfOutput = df['LapTime']\n",
    "train_targets = dfOutput.to_numpy()\n",
    "\n",
    "for i in range(len(train_targets)):\n",
    "    train_targets[i] = train_targets[i].replace('0 days ', '')\n",
    "# print(train_targets)\n",
    "actual_train_targets = []\n",
    "for time in train_targets:\n",
    "    td = parse(time) - parse('00:00:00')\n",
    "    seconds = td.total_seconds()\n",
    "    actual_train_targets.append(seconds)\n",
    "# print(actual_train_targets)\n",
    "dfLapTime = pd.DataFrame(actual_train_targets)\n",
    "\n",
    "df['LapTime'] = dfLapTime\n",
    "df = df.dropna()\n",
    "cols = ['LapTime']\n",
    "Q1 = df[cols].quantile(0.25)\n",
    "Q3 = df[cols].quantile(0.75)\n",
    "IQR = Q3-Q1\n",
    "df = df[~((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "dfInput = df.drop(['LapTime'], axis=1)\n",
    "dfInput = dfInput.replace({'SUPERHARD':1, 'HARD':2, 'MEDIUM':3, 'SOFT':4,'SUPERSOFT': 5, 'ULTRASOFT': 6,'HYPERSOFT':7,\"INTERMEDIATE\":0,\"WET\":-1})\n",
    "train_inputs = dfInput.to_numpy()\n",
    "inputs_array = train_inputs.astype('float64')\n",
    "dfOutput = df['LapTime']\n",
    "\n",
    "targets_array = dfOutput.to_numpy()\n",
    "inputs = torch.Tensor(inputs_array)\n",
    "targets = torch.Tensor(targets_array)\n",
    "\n",
    "print(inputs_array)\n",
    "print(targets_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03e8c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f1a0694",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_percent = 0.2\n",
    "num_rows = len(dfInput.index)\n",
    "val_size = int(num_rows * val_percent)\n",
    "train_size = num_rows - val_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da61690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 9\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle = True, num_workers = 0)\n",
    "val_loader = DataLoader(val_ds, batch_size, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fccaa518",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(inputs[0])\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a60896f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(11, 36),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(36,18),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(18,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.layers(xb)                       \n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        inputs, targets = batch \n",
    "        # Generate predictions\n",
    "        out = self(inputs)          \n",
    "        # Calcuate loss\n",
    "        loss = F.l1_loss(input = out, target = targets, size_average = None, reduce = None, reduction = 'mean')  \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        inputs, targets = batch\n",
    "        # Generate predictions\n",
    "        out = self(inputs)\n",
    "        # Calculate loss\n",
    "        loss = F.l1_loss(input = out, target = targets, size_average = None, reduce = None, reduction = 'mean')    \n",
    "        return {'val_loss': loss.detach()}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        return {'val_loss': epoch_loss.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result, num_epochs):\n",
    "        # Print result every 20th epoch\n",
    "        if (epoch+1) % 20 == 0 or epoch == num_epochs-1:\n",
    "            print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch+1, result['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c67ce36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc374939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-2.4807e-01,  2.9681e-01, -2.4541e-01, -2.5394e-01, -2.3929e-01,\n",
       "           2.5242e-01,  7.0965e-02, -2.6725e-01, -2.3794e-01, -1.7920e-01,\n",
       "          -1.1500e-01],\n",
       "         [ 4.0638e-02, -2.6811e-01,  2.8840e-01, -8.4375e-02,  2.9007e-01,\n",
       "           2.6344e-01,  1.3608e-01,  2.5505e-03,  2.5329e-01,  1.9598e-01,\n",
       "           5.3128e-02],\n",
       "         [-1.4135e-01, -3.2963e-02, -1.5339e-01,  1.7873e-01,  2.3644e-01,\n",
       "          -1.3322e-01, -2.1399e-01,  6.4566e-02,  1.7005e-01, -2.4731e-01,\n",
       "          -2.7327e-01],\n",
       "         [-6.6129e-02,  1.2752e-01, -1.2415e-01,  2.8823e-01,  2.9029e-01,\n",
       "          -1.2603e-01,  2.5803e-01, -2.1049e-01, -2.8530e-01, -1.1630e-01,\n",
       "           7.7897e-02],\n",
       "         [ 1.4937e-01, -5.7018e-02,  1.9241e-01, -8.1632e-03,  2.7937e-01,\n",
       "           3.9424e-02, -2.7624e-01, -1.3343e-01, -1.3487e-01, -1.6709e-01,\n",
       "           2.9074e-01],\n",
       "         [-2.4827e-01, -1.8956e-01,  1.4760e-01, -2.0358e-01, -3.1288e-02,\n",
       "           1.6744e-01,  2.4943e-01, -9.9214e-03,  9.4745e-02, -1.7643e-01,\n",
       "          -2.1948e-01],\n",
       "         [-3.0114e-01, -7.5834e-02,  2.1085e-01, -6.0994e-02,  1.9955e-01,\n",
       "           2.8314e-01, -1.5424e-01, -2.8065e-01, -6.8295e-02, -1.0436e-01,\n",
       "           1.5511e-01],\n",
       "         [-1.9045e-01, -2.2124e-01,  1.9882e-01, -1.9723e-01,  1.4407e-01,\n",
       "          -1.0002e-01,  3.9097e-02,  9.1856e-02, -2.1236e-01, -1.9221e-02,\n",
       "           2.3488e-01],\n",
       "         [-1.0751e-01, -1.4678e-02,  2.6024e-01, -2.8144e-01,  9.7213e-02,\n",
       "          -3.7394e-02, -2.1118e-01,  4.3724e-02,  1.4438e-02, -1.6461e-01,\n",
       "           1.3757e-01],\n",
       "         [-4.2683e-02, -1.3207e-01, -1.9349e-01,  2.4410e-01, -6.7187e-03,\n",
       "          -8.2260e-02,  2.8924e-01,  2.8094e-01, -8.4096e-02,  1.3616e-01,\n",
       "           2.6990e-01],\n",
       "         [-6.3554e-02,  1.8520e-01,  1.2359e-01, -1.3632e-01, -2.8662e-01,\n",
       "           6.7509e-03, -3.2355e-03,  2.1216e-01, -2.1630e-01,  4.6472e-02,\n",
       "          -2.5429e-01],\n",
       "         [ 1.6497e-01,  1.7586e-01,  2.9131e-01, -1.9659e-01, -6.8394e-02,\n",
       "           1.2731e-01,  6.5212e-02, -2.2466e-01, -6.9872e-02, -4.7301e-02,\n",
       "           8.5865e-02],\n",
       "         [-1.5134e-01,  4.6336e-02, -7.4003e-02,  5.6236e-02,  1.5753e-01,\n",
       "           2.9936e-01, -2.8539e-01,  8.7209e-02,  2.3992e-01,  8.1213e-02,\n",
       "           2.3819e-01],\n",
       "         [-1.5117e-01, -1.7055e-01,  1.1759e-01,  2.6529e-02,  2.3343e-01,\n",
       "           1.7105e-01, -2.2449e-01,  2.3333e-01, -2.6439e-01, -1.7452e-01,\n",
       "          -1.7922e-01],\n",
       "         [ 1.9713e-01, -2.6497e-01, -1.1050e-01, -1.4733e-01,  2.0655e-01,\n",
       "          -2.8504e-01,  2.9827e-01, -2.2281e-01, -5.9897e-02,  2.1104e-01,\n",
       "           2.0054e-01],\n",
       "         [-1.3983e-01,  1.3563e-01, -1.1702e-01,  2.9053e-01, -2.3689e-01,\n",
       "          -1.5672e-01,  2.4924e-01, -2.9463e-01, -7.0889e-03,  8.9160e-02,\n",
       "           2.8879e-02],\n",
       "         [-2.8959e-01,  3.4563e-02,  1.7223e-01, -9.0459e-02, -1.4977e-01,\n",
       "          -6.0712e-02,  1.8250e-01,  1.0989e-01, -8.8264e-02, -2.0386e-01,\n",
       "           3.0532e-02],\n",
       "         [ 7.1954e-02,  1.1372e-01, -1.2686e-02, -5.2251e-02,  2.5146e-01,\n",
       "           1.8220e-01,  2.9678e-01,  1.0975e-01,  1.2531e-01,  1.4605e-01,\n",
       "          -1.9592e-01],\n",
       "         [-9.3632e-02,  1.3045e-01,  2.3126e-01,  2.8738e-02,  2.5924e-01,\n",
       "           2.0914e-01,  2.7414e-01,  4.2045e-02,  9.1993e-02, -2.3617e-01,\n",
       "          -1.3853e-02],\n",
       "         [-2.8728e-01, -6.2968e-02, -2.0134e-01, -1.8559e-01, -1.4210e-01,\n",
       "           1.1403e-01,  1.9460e-02,  2.0845e-01,  1.7652e-01, -1.2387e-01,\n",
       "           2.7194e-01],\n",
       "         [-1.5877e-01,  1.2650e-01, -3.1727e-02,  2.7806e-01,  1.0095e-01,\n",
       "          -2.2651e-02, -4.5350e-02, -2.1205e-01, -9.1286e-02,  7.2470e-03,\n",
       "           2.6883e-01],\n",
       "         [-1.3653e-01, -1.5787e-01, -2.0963e-02,  6.5338e-03, -1.6868e-01,\n",
       "          -5.1128e-02, -9.8485e-02,  1.7809e-01,  1.9109e-01, -1.7223e-01,\n",
       "           1.9420e-01],\n",
       "         [ 1.3870e-01,  2.8013e-01,  7.0408e-02,  8.5524e-02,  1.5351e-01,\n",
       "          -1.5845e-01, -2.2646e-01,  2.2582e-01,  1.7413e-01, -2.6209e-01,\n",
       "           1.3652e-04],\n",
       "         [-1.1927e-02, -2.2103e-01, -2.4557e-01, -2.1070e-01, -1.7821e-01,\n",
       "          -1.2736e-01,  7.4438e-03, -1.9505e-01, -2.1174e-01,  2.3211e-01,\n",
       "           7.8218e-02],\n",
       "         [-2.6836e-01,  2.4277e-01,  6.4359e-02,  9.1070e-02, -1.6551e-02,\n",
       "          -8.0103e-02, -2.3287e-02, -1.7751e-01,  1.4245e-01,  1.6311e-01,\n",
       "          -3.9073e-02],\n",
       "         [ 4.5501e-02, -2.5281e-01,  2.8776e-01, -2.3021e-01,  2.7258e-01,\n",
       "           1.2538e-01, -2.2526e-01,  2.9337e-01, -2.7666e-01,  7.7458e-02,\n",
       "           1.8073e-01],\n",
       "         [ 2.5002e-01, -3.2687e-02,  6.4230e-02, -2.5437e-02,  2.6070e-01,\n",
       "          -7.8084e-02, -1.4698e-01, -1.5563e-01, -5.6408e-02,  1.4874e-01,\n",
       "           3.7218e-02],\n",
       "         [-9.7863e-02, -1.0177e-01, -2.3966e-01,  5.4313e-02, -1.8601e-01,\n",
       "           7.4293e-02,  1.1105e-01,  1.1263e-01,  7.2146e-02, -2.2212e-01,\n",
       "          -2.0511e-01],\n",
       "         [ 1.6403e-01,  8.2031e-02,  5.9272e-02, -2.6391e-01,  2.4076e-01,\n",
       "          -2.2786e-01, -2.4001e-01,  1.9360e-01, -1.3290e-02, -1.9365e-01,\n",
       "           3.8910e-02],\n",
       "         [-4.8368e-02,  2.1884e-01,  1.1027e-01,  2.1940e-01, -1.6297e-01,\n",
       "          -2.1398e-01,  8.5426e-02,  1.7420e-01, -9.0760e-02, -5.3169e-03,\n",
       "          -1.7705e-01],\n",
       "         [-1.8617e-01,  8.0679e-02,  3.3606e-02,  1.4499e-01,  2.2715e-01,\n",
       "           2.3253e-01,  1.3417e-01,  2.3141e-01, -8.8096e-02,  1.3959e-01,\n",
       "           1.1335e-01],\n",
       "         [ 2.1404e-03, -2.9632e-01, -1.5442e-01,  3.6233e-02, -2.1770e-02,\n",
       "          -2.7250e-01,  3.8444e-02, -2.8000e-01, -1.3090e-01,  2.5512e-01,\n",
       "          -9.1150e-02],\n",
       "         [-2.0618e-02,  2.0535e-03, -9.9177e-02, -1.6679e-01, -2.4013e-01,\n",
       "           1.3679e-01,  2.7455e-01,  4.3772e-02, -1.3467e-01, -2.1642e-01,\n",
       "          -1.9229e-01],\n",
       "         [ 2.9747e-01,  2.9157e-01,  1.3945e-01, -1.7557e-01, -4.4728e-02,\n",
       "          -7.4060e-02, -2.5514e-01, -2.9563e-01, -1.4738e-01,  2.4425e-01,\n",
       "          -8.3847e-02],\n",
       "         [-8.6559e-02, -1.6825e-01,  9.0530e-02,  8.7152e-02,  1.9579e-01,\n",
       "           5.6025e-03, -4.3761e-02, -1.6606e-01,  2.6775e-01,  2.8557e-01,\n",
       "           1.7999e-01],\n",
       "         [ 1.0367e-01,  2.4031e-01,  2.7744e-01, -1.7001e-01, -1.8130e-01,\n",
       "           2.5135e-01,  1.7863e-01,  3.3818e-02,  1.7907e-01, -1.0878e-01,\n",
       "          -8.2842e-02]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1546,  0.2778,  0.0881,  0.1351, -0.0564,  0.3004,  0.2848, -0.1863,\n",
       "         -0.0363,  0.1729, -0.2071, -0.1695, -0.2757,  0.2055, -0.3004,  0.0546,\n",
       "         -0.0982,  0.0063,  0.2639,  0.1027,  0.2892, -0.2006,  0.0862,  0.1725,\n",
       "          0.2034,  0.2229,  0.1788, -0.2581,  0.0831, -0.2377,  0.0989,  0.0834,\n",
       "          0.2655,  0.1296, -0.0801, -0.0013], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0519, -0.1412, -0.1279,  0.0579, -0.0059,  0.0509,  0.0756,  0.0152,\n",
       "          -0.0922,  0.0543,  0.1302, -0.1376,  0.1491,  0.0034,  0.0555, -0.0886,\n",
       "           0.1564, -0.0753, -0.0834, -0.0902,  0.0371, -0.1085,  0.0431, -0.1502,\n",
       "          -0.0375, -0.1129,  0.1357,  0.0383, -0.1427,  0.1313, -0.0971, -0.1282,\n",
       "           0.1318,  0.0721, -0.1575,  0.0974],\n",
       "         [ 0.1035, -0.0942,  0.0698,  0.0451,  0.1038, -0.0100,  0.1051,  0.1322,\n",
       "          -0.0098, -0.0415, -0.0926,  0.0567, -0.1575,  0.1612, -0.0787,  0.1139,\n",
       "           0.0854, -0.1153,  0.0423,  0.0342, -0.1416, -0.0121,  0.0051, -0.0933,\n",
       "          -0.0938,  0.0983, -0.0388,  0.1247,  0.0225,  0.1249,  0.0982,  0.0782,\n",
       "           0.1640, -0.1572,  0.1374,  0.0463],\n",
       "         [-0.0838,  0.1198, -0.0150, -0.0165,  0.1291,  0.1176,  0.1446, -0.1338,\n",
       "           0.0505,  0.0750,  0.0564,  0.1337,  0.0400, -0.0420,  0.0620,  0.1031,\n",
       "          -0.1184,  0.0703, -0.0773, -0.0148,  0.0413, -0.0699, -0.0536,  0.1127,\n",
       "          -0.0791, -0.0123,  0.0585, -0.1268,  0.1593,  0.1476,  0.1518, -0.1135,\n",
       "          -0.0068,  0.1382,  0.1507, -0.0640],\n",
       "         [ 0.0486, -0.0551,  0.0005,  0.0797, -0.0880, -0.0108, -0.1089, -0.0463,\n",
       "           0.0112,  0.1486, -0.0780,  0.1297, -0.0542,  0.1258,  0.1540,  0.1193,\n",
       "          -0.0497,  0.0857,  0.0098, -0.0557, -0.1372,  0.0612,  0.0161,  0.0744,\n",
       "           0.0614,  0.0739, -0.1657,  0.1425,  0.1219,  0.0497,  0.0365, -0.1056,\n",
       "          -0.1561, -0.1304, -0.1631,  0.1131],\n",
       "         [ 0.1284, -0.0337, -0.0724, -0.0002, -0.0312, -0.0297,  0.0269,  0.0977,\n",
       "          -0.0911, -0.1367,  0.0226, -0.0688, -0.0845,  0.0514,  0.0338,  0.0518,\n",
       "           0.1162,  0.1111,  0.0079, -0.0331,  0.0222,  0.0383,  0.1560, -0.1332,\n",
       "          -0.1156, -0.1033, -0.0337, -0.0162,  0.1046, -0.1539,  0.0101,  0.1293,\n",
       "          -0.1638, -0.0263,  0.0185, -0.1018],\n",
       "         [ 0.0657, -0.0647, -0.1175, -0.0007,  0.0775, -0.0672, -0.0996, -0.0496,\n",
       "          -0.1457, -0.0346, -0.1559, -0.0986, -0.0761, -0.0739,  0.1517,  0.0246,\n",
       "          -0.0594,  0.1434, -0.0972,  0.1193,  0.1514, -0.0688, -0.0994,  0.1417,\n",
       "          -0.1444,  0.1420, -0.0204, -0.0392, -0.1170, -0.1290,  0.0309, -0.0758,\n",
       "          -0.0741, -0.0213, -0.0831,  0.0873],\n",
       "         [ 0.0973, -0.1185,  0.0976,  0.1501, -0.1432, -0.0877,  0.0169, -0.0596,\n",
       "          -0.0428, -0.1236, -0.1423,  0.0237, -0.1334,  0.0722,  0.1070,  0.1369,\n",
       "           0.0551,  0.0996, -0.0639,  0.1447,  0.0237, -0.0495, -0.1116, -0.1525,\n",
       "           0.0503,  0.0270,  0.0134,  0.0315,  0.0458,  0.0560, -0.1384,  0.1216,\n",
       "          -0.0139,  0.1443, -0.1658,  0.0304],\n",
       "         [-0.0378, -0.0491, -0.1257,  0.1088,  0.1160,  0.1232,  0.1656, -0.1250,\n",
       "          -0.0688,  0.1445,  0.1220,  0.0800,  0.0018, -0.0518, -0.0456, -0.0195,\n",
       "          -0.1147,  0.1547,  0.1256, -0.0677,  0.0132, -0.0450, -0.1473, -0.1557,\n",
       "           0.0511,  0.0042,  0.1635, -0.1537,  0.1164,  0.1536,  0.0355, -0.0471,\n",
       "          -0.0607,  0.0508, -0.1554,  0.0287],\n",
       "         [ 0.1647,  0.0724,  0.0656,  0.0784,  0.0787, -0.1490,  0.1202, -0.1208,\n",
       "           0.0152,  0.1125,  0.1504,  0.0578,  0.1415,  0.0695, -0.0316, -0.0082,\n",
       "          -0.0999, -0.0066,  0.0689, -0.1301,  0.1193,  0.0023,  0.1132, -0.0027,\n",
       "           0.0639,  0.0958, -0.1261, -0.1211, -0.1155, -0.1251, -0.1308,  0.0153,\n",
       "          -0.0599,  0.1384, -0.0695, -0.0408],\n",
       "         [-0.1609, -0.0834, -0.0832, -0.0766, -0.0162, -0.0126, -0.0116,  0.0632,\n",
       "          -0.0103, -0.1226,  0.0806,  0.0586, -0.0085,  0.1190,  0.1640, -0.0457,\n",
       "           0.0791, -0.1283,  0.1360,  0.0815, -0.1594, -0.0263,  0.0323, -0.0827,\n",
       "          -0.1361,  0.0682, -0.0684,  0.0080,  0.0123,  0.1013,  0.1141,  0.1499,\n",
       "           0.0840,  0.0251,  0.0406,  0.0043],\n",
       "         [-0.0187, -0.1170,  0.1026, -0.1308, -0.0622,  0.1198,  0.1136,  0.1625,\n",
       "          -0.0725,  0.0827, -0.0322, -0.0458,  0.0153,  0.0834, -0.0761, -0.1350,\n",
       "          -0.1499,  0.0341,  0.1049,  0.1243, -0.1475,  0.1161,  0.1538,  0.0151,\n",
       "           0.0290,  0.0808, -0.0340,  0.0026, -0.0691,  0.1046,  0.1406, -0.1537,\n",
       "          -0.0444,  0.0321, -0.0062,  0.1650],\n",
       "         [-0.0915,  0.1177,  0.0717, -0.0240, -0.0202, -0.0944,  0.0697,  0.0149,\n",
       "           0.0385, -0.0324, -0.0515, -0.1203,  0.0946,  0.1354, -0.0074,  0.0067,\n",
       "          -0.0558, -0.0186, -0.0372,  0.1022, -0.0214, -0.0248, -0.1303, -0.1467,\n",
       "          -0.1344,  0.0635, -0.0216,  0.0489, -0.0822, -0.1473,  0.0167,  0.0994,\n",
       "          -0.0588, -0.0356, -0.0702, -0.0462],\n",
       "         [ 0.1594,  0.0209, -0.0393,  0.1297,  0.0116, -0.0990,  0.1400,  0.0524,\n",
       "           0.0474, -0.0883,  0.1010,  0.1072,  0.0079, -0.1283,  0.0039,  0.1084,\n",
       "          -0.0749, -0.0492, -0.0619, -0.1625, -0.1016, -0.0923, -0.1434,  0.1650,\n",
       "           0.0230, -0.0681,  0.0516,  0.1608,  0.0518, -0.0936,  0.1427,  0.0095,\n",
       "           0.0964, -0.1323, -0.0008, -0.1229],\n",
       "         [ 0.0204,  0.1547, -0.0819,  0.0230, -0.0455,  0.0426, -0.1657,  0.0549,\n",
       "           0.1172,  0.1525,  0.0473, -0.0326, -0.0977,  0.1141, -0.0957,  0.0661,\n",
       "           0.1204,  0.0840, -0.1553,  0.0940,  0.0253, -0.0852,  0.1251, -0.1563,\n",
       "           0.0713, -0.1568, -0.0775, -0.0840, -0.1107,  0.0851,  0.0847, -0.1311,\n",
       "           0.0597,  0.0560, -0.0889,  0.0014],\n",
       "         [ 0.0636,  0.0272, -0.0293, -0.1611,  0.0102, -0.1213, -0.1241, -0.1660,\n",
       "           0.1137,  0.1537, -0.0908, -0.0796, -0.1464, -0.1488, -0.1199,  0.0964,\n",
       "           0.1285, -0.0957, -0.1174,  0.0506, -0.0344,  0.1481, -0.0030,  0.1150,\n",
       "           0.1524,  0.0680,  0.1113,  0.1561,  0.1354,  0.1602,  0.0988,  0.1123,\n",
       "          -0.1540, -0.1074, -0.0611,  0.0835],\n",
       "         [ 0.0136,  0.0869, -0.0464,  0.1144,  0.0236, -0.0668, -0.0381,  0.0170,\n",
       "           0.0089, -0.0504, -0.1125,  0.1330, -0.0569,  0.0027, -0.0909,  0.1601,\n",
       "           0.1485, -0.0124,  0.1066, -0.1605,  0.0975,  0.1417,  0.0291,  0.1330,\n",
       "          -0.0924,  0.0968, -0.1425,  0.0970,  0.1321, -0.1439,  0.0633, -0.0363,\n",
       "           0.1471, -0.0825,  0.0414,  0.1448],\n",
       "         [-0.0740,  0.0138, -0.0627, -0.0565, -0.0792,  0.1180,  0.0894, -0.0881,\n",
       "          -0.0318, -0.0870,  0.1608,  0.1664, -0.0077,  0.0160, -0.1180,  0.1598,\n",
       "           0.0445, -0.0342, -0.1613,  0.0357, -0.1319,  0.0703,  0.0598, -0.1083,\n",
       "          -0.0590,  0.1024, -0.1389, -0.0627,  0.0835,  0.1490,  0.1531,  0.1462,\n",
       "           0.0117,  0.0019,  0.0485, -0.0588],\n",
       "         [-0.1091, -0.0212, -0.0727,  0.1130,  0.1071,  0.0365,  0.1453, -0.0346,\n",
       "           0.0024, -0.0780,  0.0303, -0.1207,  0.0441, -0.0017,  0.1583,  0.0039,\n",
       "           0.1041, -0.1284,  0.1439,  0.0699,  0.0198,  0.0912,  0.1167,  0.1527,\n",
       "           0.0655, -0.1301, -0.1036, -0.1074, -0.1491,  0.0895,  0.0067,  0.1653,\n",
       "           0.1576,  0.1377, -0.1315,  0.0962]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0024, -0.0935,  0.1530, -0.0643, -0.1427,  0.1126, -0.1042,  0.0368,\n",
       "          0.0908, -0.1136,  0.0506,  0.1644,  0.0283, -0.1514,  0.0861, -0.1196,\n",
       "         -0.0799, -0.1526], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1114, -0.1509, -0.1160,  0.1054,  0.2077,  0.1927,  0.1174,  0.1198,\n",
       "           0.0932,  0.1107,  0.2054, -0.2196, -0.0654, -0.1086, -0.2235, -0.1191,\n",
       "           0.0840,  0.1062]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1884], requires_grad=True)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c8906d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result, epochs)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8c986ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp/ipykernel_6468/1372700796.py:29: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.l1_loss(input = out, target = targets, size_average = None, reduce = None, reduction = 'mean')\n",
      "C:\\Temp/ipykernel_6468/1372700796.py:29: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.l1_loss(input = out, target = targets, size_average = None, reduce = None, reduction = 'mean')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 85.80325317382812}\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(model, val_loader)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a3ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp/ipykernel_6468/1372700796.py:21: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.l1_loss(input = out, target = targets, size_average = None, reduce = None, reduction = 'mean')\n",
      "C:\\Temp/ipykernel_6468/1372700796.py:21: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.l1_loss(input = out, target = targets, size_average = None, reduce = None, reduction = 'mean')\n",
      "C:\\Temp/ipykernel_6468/1372700796.py:29: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.l1_loss(input = out, target = targets, size_average = None, reduce = None, reduction = 'mean')\n",
      "C:\\Temp/ipykernel_6468/1372700796.py:29: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.l1_loss(input = out, target = targets, size_average = None, reduce = None, reduction = 'mean')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 11.6690\n",
      "Epoch [40], val_loss: 11.6450\n",
      "Epoch [60], val_loss: 11.6357\n",
      "Epoch [80], val_loss: 11.6316\n",
      "Epoch [100], val_loss: 11.6297\n",
      "Epoch [120], val_loss: 11.6243\n",
      "Epoch [140], val_loss: 11.6223\n",
      "Epoch [160], val_loss: 11.6181\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "lr = 1e-7\n",
    "history1 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18488a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(input, target, model):\n",
    "    inputs = input.unsqueeze(0)\n",
    "    predictions = model(inputs)                \n",
    "    prediction = predictions[0].detach()\n",
    "    return target, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603f4338",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, target = val_ds[11]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4dcbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, target = val_ds[50]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bd0a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "predictions = []\n",
    "for i in range(len(val_ds)):\n",
    "    input, target = val_ds[i]\n",
    "    target, prediction = predict_single(input, target, model)\n",
    "    targets.append(target)\n",
    "    predictions.append(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb1ef5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "ax.scatter(range(len(predictions)),targets)\n",
    "ax.scatter(range(len(predictions)), predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc3ce23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9964388e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9f1f59101e07bffb7c2ecfaca1a3c7ffe3cd326ee75e914ab1b038684b38c5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
